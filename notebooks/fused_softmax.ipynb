{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d8a83e0",
      "metadata": {
        "id": "3d8a83e0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import triton\n",
        "import triton.language as tl\n",
        "from triton.runtime import driver\n",
        "DEVICE = triton.runtime.driver.active.get_active_torch_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fed7ac6a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fed7ac6a",
        "outputId": "76313b90-c9f8-4746-bae2-8d9acfe67cdf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def naive_softmax(x:torch.Tensor) -> torch.Tensor:\n",
        "    # N (row) * M (col) softmax\n",
        "    x_max = x.max(dim=1)[0] # NM reads (max of each row), M writes\n",
        "    z = x - x_max[:,None] #  NM writes, NM + M read\n",
        "    numerator = torch.exp(z) # NM reads, NM writes\n",
        "    denominator = numerator.sum(dim=1) # N writes, NM reads\n",
        "    ret = numerator / denominator[:,None] # NM writes, NM + m reads\n",
        "    return ret\n",
        "\n",
        "SIZE =5\n",
        "x = torch.rand([SIZE,SIZE])\n",
        "x_out = naive_softmax(x)\n",
        "\n",
        "def assert_softmax_row_sum_one(x):\n",
        "    return torch.allclose(x.sum(dim=1),torch.ones_like(x.sum(dim=1)))\n",
        "assert_softmax_row_sum_one(x_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "62b56e9a",
      "metadata": {
        "id": "62b56e9a"
      },
      "outputs": [],
      "source": [
        "@triton.jit\n",
        "def softmax_kernel(output_ptr,input_ptr,input_row_strides,output_row_strides,n_rows,n_cols,BLOCK_SIZE:tl.constexpr,num_stages:tl.constexpr):\n",
        "    #idx of starting row\n",
        "    row_start = tl.program_id(axis=0)\n",
        "    #num of rows until the next program (row_start + row_step*input_row_strides)\n",
        "    row_step = tl.num_programs(axis=0)\n",
        "\n",
        "    for row_idx in tl.range(row_start,n_rows,row_step,num_stages=num_stages):\n",
        "        row_start_ptr = input_ptr + (row_idx * input_row_strides) # start of data + the offset to get to the current read\n",
        "        #NOTE: limitation of triton is block sizes must be a power of 2. e.g block size is always the next bigger power of 2 value above the amount you need\n",
        "        # so in this case it will be the next power of 2 above num of columns\n",
        "        col_offsets = tl.arange(0,BLOCK_SIZE)\n",
        "\n",
        "        # create ptrs to each element within the current row\n",
        "        input_ptrs = row_start_ptr + col_offsets\n",
        "\n",
        "        mask = col_offsets < n_cols\n",
        "\n",
        "        row = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n",
        "        row_minus_max = row - tl.max(row, axis=0)\n",
        "        # NOTE: exp in tl is approximate (similar to __expf in cuda)\n",
        "        numerator = tl.exp(row_minus_max)\n",
        "        denominator = tl.sum(numerator,axis=0)\n",
        "        softmax_out = numerator / denominator\n",
        "        output_row_start_ptr = output_ptr + (row_idx * output_row_strides)\n",
        "        output_ptrs = output_row_start_ptr +  col_offsets\n",
        "        tl.store(output_ptrs,softmax_out,mask=mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "9132fd6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "@triton.jit\n",
        "def softmax_kernel_single_row(output_ptr,input_ptr,input_row_strides,output_row_strides,n_rows,n_cols,BLOCK_SIZE:tl.constexpr,num_stages:tl.constexpr):\n",
        "    #idx of starting row\n",
        "    row_start = tl.program_id(axis=0)\n",
        "    data_start = row_start * n_cols\n",
        "\n",
        "    col_offsets = tl.arange(0,BLOCK_SIZE)\n",
        "\n",
        "    input_ptrs = input_ptr + (row_start * input_row_strides) + col_offsets\n",
        "    mask = col_offsets < n_cols\n",
        "\n",
        "    row = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n",
        "    row_minus_max = row - tl.max(row, axis=0)\n",
        "\n",
        "    numerator = tl.exp(row_minus_max)\n",
        "    denominator = tl.sum(numerator,axis=0)\n",
        "    softmax_out = numerator / denominator\n",
        "    output_ptrs = output_ptr + (row_start * output_row_strides) +   col_offsets\n",
        "    tl.store(output_ptrs,softmax_out,mask=mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "5c4adb44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c4adb44",
        "outputId": "43f59962-1ee7-4793-d9e2-dc6a40e495af"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (775748212.py, line 50)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[113]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mkernel[(n_rows, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_S~IZE, num_stages)\u001b[39m\n                                                                           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ],
      "source": [
        "from enum import StrEnum\n",
        "\n",
        "properties = driver.active.utils.get_device_properties(DEVICE.index)\n",
        "print(f'{properties=}')\n",
        "\n",
        "NUM_SM = properties['multiprocessor_count']\n",
        "NUM_REGS = properties['max_num_regs']\n",
        "SIZE_SMEM = properties['max_shared_mem']\n",
        "WARP_SIZE = properties['warpSize']\n",
        "target = triton.runtime.driver.active.get_current_target()\n",
        "print(f'{target=}')\n",
        "kernels = {}\n",
        "\n",
        "\n",
        "class SoftmaxKernel(StrEnum):\n",
        "    \"\"\"Kernel selection: ROW_BLOCK = multi-row per program, SINGLE_ROW = one row per program.\"\"\"\n",
        "    ROW_BLOCK = \"row_block\"\n",
        "    SINGLE_ROW = \"single_row\"\n",
        "\n",
        "\n",
        "def softmax(x, kernel_type: SoftmaxKernel = SoftmaxKernel.SINGLE_ROW):\n",
        "    n_rows, n_cols = x.shape\n",
        "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
        "\n",
        "    num_warps = 8\n",
        "    num_stages = 4 if SIZE_SMEM > 200_000 else 2\n",
        "\n",
        "    y = torch.empty_like(x)\n",
        "\n",
        "    if kernel_type == SoftmaxKernel.SINGLE_ROW:\n",
        "        kernel_fn = softmax_kernel_single_row\n",
        "        warmup_grid = (1,)\n",
        "    else:\n",
        "        kernel_fn = softmax_kernel\n",
        "        warmup_grid = (1,)\n",
        "\n",
        "    kernel = kernel_fn.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE, num_stages=num_stages, num_warps=num_warps, grid=warmup_grid)\n",
        "    kernel._init_handles()\n",
        "    n_regs = kernel.n_regs\n",
        "    size_smem = kernel.metadata.shared\n",
        "\n",
        "    occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)\n",
        "    if size_smem > 0:\n",
        "        occupancy = min(occupancy, SIZE_SMEM // size_smem)\n",
        "\n",
        "    num_programs = NUM_SM * occupancy\n",
        "    num_programs = min(num_programs, n_rows)\n",
        "\n",
        "    if kernel_type == SoftmaxKernel.SINGLE_ROW:\n",
        "        kernel[(n_rows, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_S~IZE, num_stages)\n",
        "    else:\n",
        "        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)\n",
        "    return y\n",
        "\n",
        "x = torch.randn([320, 320], device=DEVICE)\n",
        "y = softmax(x)\n",
        "assert_softmax_row_sum_one(softmax(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "81e4fbf7",
      "metadata": {
        "id": "81e4fbf7"
      },
      "outputs": [],
      "source": [
        "def test_softmax_padding():\n",
        "    torch.manual_seed(0)\n",
        "    x = torch.randn(1823, 781, device=DEVICE)\n",
        "    y_triton = softmax(x, kernel_type=SoftmaxKernel.ROW_BLOCK)\n",
        "    y_torch = torch.softmax(x, axis=1)\n",
        "    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)\n",
        "test_softmax_padding()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b864ad82",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b864ad82",
        "outputId": "587ec585-266e-4f15-93c3-c859d8d2378c"
      },
      "outputs": [],
      "source": [
        "@triton.testing.perf_report(\n",
        "    triton.testing.Benchmark(\n",
        "        x_names=['N'],\n",
        "        x_vals=[128 * i for i in range (2,10)],\n",
        "        line_arg='provider',\n",
        "        line_vals=['triton-multi-row','triton-single-row','torch','naive'],\n",
        "        line_names=['triton-multi-row','triton-single-row','torch','naive'],\n",
        "        styles = [('yellow','-'),('blue','-'),('green','-'),('red','-')],\n",
        "        ylabel='GB/s',\n",
        "        plot_name = 'softmax-performance',\n",
        "        args={'M':4096}\n",
        "    )\n",
        ")\n",
        "def benchmark(M,N,provider):\n",
        "    quantiles = [0.5,0.2,0.8]\n",
        "    x=torch.randn(M,N,device=DEVICE,dtype=torch.float32)\n",
        "    stream = getattr(torch, DEVICE.type).Stream()\n",
        "    getattr(torch, DEVICE.type).set_stream(stream)\n",
        "    if provider == 'torch':\n",
        "        ms,min_ms,max_ms = triton.testing.do_bench(lambda: torch.softmax(x,axis=1),quantiles=quantiles)\n",
        "    if provider == 'triton-single-row':\n",
        "        ms,min_ms,max_ms = triton.testing.do_bench(lambda: softmax(x,kernel_type=SoftmaxKernel.SINGLE_ROW),quantiles=quantiles)\n",
        "    if provider == 'triton-multi-row':\n",
        "        ms,min_ms,max_ms = triton.testing.do_bench(lambda: softmax(x,kernel_type=SoftmaxKernel.ROW_BLOCK),quantiles=quantiles)\n",
        "    if provider == 'naive':\n",
        "        ms,min_ms,max_ms = triton.testing.do_bench(lambda: naive_softmax(x),quantiles=quantiles)\n",
        "    gbps = lambda ms: 2*x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
        "    return gbps(ms),gbps(min_ms),gbps(max_ms)\n",
        "\n",
        "benchmark.run(show_plots=True,print_data=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
